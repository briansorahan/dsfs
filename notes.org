* Unit 4
** Cross Validation (CV)
On the scikit-learn website they talk about knowledge of the test set "leaking"
into the model. I believe this happens when we do the following:
(1) Train the model on set A.
(2) Test the model on set B.
(3) Use the results of (2) to generate the next iteration of the model that we feed back into (1).
This can result in overfitting because the model will now predict both the training and testing
sets perfectly but could easily fail on real world data.
How do we avoid this?

One way is to partition the data into 3 sets: training, testing, and validation.
You then train on the training set, validate on the validation set, and when you think the
model is finally done you test on the testing set.
What if the model still performed poorly on the testing set?
You would avoid training it on the testing set, so at the point I guess you would have to
think if a different way to model the predictions.

Another solution is Cross Validation.
In particular, we can use k-fold CV.
k-fold CV entails splitting the training set into k sets (also called "folds"), then
(1) A model is trained using k-1 of the folds.
(2) The model is validated against the remaining fold.
The performance is then computed as the average of the performance measurements
across all the choices of training and validation sets.
